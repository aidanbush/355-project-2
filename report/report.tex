\documentclass[12pt]{article}
\usepackage{fullpage}
\usepackage[top=2cm, bottom=4.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{fancyhdr}

\pagestyle{fancyplain}
\headheight 35pt
\lhead{Aidan Bush, David Dowie, Ben Ha}
\rhead{CMPT 355 \\ \today}
\headsep 1.5em

\begin{document}

\section*{Project 2-Konane}

\subsection*{Implementation}
In our approach we used Alpha-Beta pruning with iterative deepening.
Iterative deepening was used so that we are able better work within the time limit.
The iterative deepening also resulted in our agent in exploring an entire depth of possible moves.
We also sorted potential moves by their evaluation values to allow for Alpha-Beta pruning.

% timing
To time our agent, and ensure It had a move in the time allotted we used a separate thread with a timer.
Before we started our search for each move we would create the thread to be our timer.
The thread would sleep for the time allotted for the move, and then would wake up and set a flag to signify that the time was up.
The search would check the timer flag and then recurse back out of the iterative deepening loop before printing off the move.

To represent our state space we used a multi-dimensional array of 8 by 8 one byte integers.
We used this model to allow easy access to each of the boards entries, and to simplify development.

\subsection*{Enhancements}
% saving moves
To improve out approach we kept all moves throughout our successive moves.
This allowed us to not have to re-search all the moves we had for our previous moves.
Saving moves also allowed us to search deeper that if we had not as we would continue off on the search we stopped on.
Specifically the agent would start searching at 3 less than the depth we stopped, to account for our, and our opponents move, and then would continue off on the last depth we were searching at.
This improvement did have a downside, in that we would always generate all possible moves from a specific depth.
If we did not we would have to constantly check for moves we left out when searching for our next move.
Although this could be done in the time our opponent spent making a move.

% timer in separate thread
We implemented our timer in a separate thread from our main search.
Using a second thread made checking the time easier, as we could just used a flag that would be set when the time was up.
Using the flag resulted in searching though more moves and finding better moves, along with winning moves quicker.
The second thread also slept which meant that it would not take up time from our main thread.

% compiler optimization

\subsection*{Problems}
% printing off and selected differing moves

% missed leftmost column and topmost row moves

\subsection*{Heuristic}
For our evaluation function we considered various ways of evaluating the given state in a way that would help us generate a optimal solution.
In the end we decided to use our Difference in moves heuristic as it preformed the best, when compared to the others.

\begin{itemize}
\item \textbf{Difference in moves} -
Takes in the given state and calculates the number of moves for black and white then takes the difference of the two. 
This would provide our minmax algorithm because it would return a larger value for when there are more available moves for black than white and a smaller value for the inverse.
This would provide both min and max with their respective optimal decisions.
The problem with using this heuristic is that it would not provide a big range between values resulting in more than one state returning the same value.

\item \textbf{Remaining stones} -
Takes the given state and counts the amount of remaining stones for both white and black.
Taking the difference between the two counts would result in the minmax algorithm would return larger values for when the state has more black than white.
In the other case, for min, the evaluation would return smaller values for when there are more white than black.

\item \textbf{Combined heuristic} - 
% TODO add

\end{itemize}

\subsection*{Future Work}
If we had the time, we would have liked to make a few improvements.
Firstly we would introduce a memory pool for our states and their arrays of children.
A memory pool would allow us to make less memory allocating system calls.
This would then reduce the number of context switches and letting our program to spend more time on the CPU.
This would allow our agent to spend more time exploring nodes, so that we could find more optimal paths to take.

Another improvement we would make, would be to modify our board representation to use just a single 64 bit integer.
Ideally we would union this with an array of 8 8 bit integers for easier interfacing with the representation.
This reduction is space used should speed up our agent as it would have to access less memory.
The memory would also be less spread allowing it to take advantage of the principal of spacial locality, which modern caching systems are built on.

Our most effective heuristic included calculating the number of moves possible for both players.
If we modified our agent to only support this heuristic, we would know exactly how many children each node would have.
With this number it would then not have to resize its array of children, reducing the time we spent resizing arrays as we grew the tree of moves.
Improvements from this agent would likely be minimal though, so it may be better to not add this as it severely limited the types of heuristics you can test.

\end{document}
